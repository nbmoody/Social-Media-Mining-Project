{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Data Acquisition Script\n",
    "### Acquiring data in fulfillment of the final project for ILS Z-639, \"Social Media Mining\"\n",
    "The following script will be run three times each day, over the course of seven sequential days, gathering 1400 Twitter Statuses each time and appending them to a dataset. To accomplish this, the script will execute the following tasks:\n",
    "\n",
    "1. Access the Twitter API.\n",
    "2. Create data storage and execute search for Twitter Statuses.\n",
    "    * Execute a search on the school's name.\n",
    "    * Use the cursor object to account for rate-limiting, and accumulate Twitter Statuses in a temporary dictionary.\n",
    "    * Append the Twitter Statuses gathered this way to the accumulator dataframe.\n",
    "3. Write/append the accumulator dataframe to file.\n",
    "4. Check the state of the dataset, and create a new backup copy each time the script is executed.\n",
    "\n",
    "Once the data-set is accumulated, it can be accessed by other scripts for cleaning/prep and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Access the Twitter API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy as tp\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Twitter_Keys.json\", \"r\") as file:\n",
    "    keys = json.load(file)\n",
    "\n",
    "API_KEY = keys[\"API_KEY\"]\n",
    "API_SECRET = keys[\"API_SECRET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tp.AppAuthHandler(API_KEY, API_SECRET)\n",
    "api = tp.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Storage Creation and Search/Accumulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigten = [\n",
    "    \"Indiana University\",\n",
    "    \"Michigan State University\",\n",
    "    \"Northwestern University\",\n",
    "    \"Ohio State University\",\n",
    "    \"Penn State\",\n",
    "    \"Purdue University\",\n",
    "    \"Rutgers University\",\n",
    "    \"University of Maryland\",\n",
    "    \"University of Illinois\",\n",
    "    \"University of Iowa\",\n",
    "    \"University of Michigan\",\n",
    "    \"University of Minnesota\",\n",
    "    \"University of Nebraska\",\n",
    "    \"University of Wisconsin\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(columns=[\"School\", \"Text\", \"Location\", \"Time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for school in bigten:\n",
    "    c = tp.Cursor(api.search, q=school, lang=\"en\")\n",
    "    for status in c.items(100):\n",
    "        tempDict = {\"School\":school, \"Text\": status.text, \"Location\":status.geo, \"Time\":status.created_at}\n",
    "        df = df.append(tempDict, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write/Append Data to a File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the dataframe for the first time this file is created.\n",
    "projectData = pd.DataFrame(columns=[\"School\", \"Text\", \"Location\", \"Time\"])\n",
    "\n",
    "# Check if the file exists. If so, load in the current state of the data.\n",
    "if os.path.exists(\"ProjectData.pkl\"):\n",
    "    with open(\"ProjectData.pkl\", \"rb\") as fileData:\n",
    "        projectData = pkl.load(fileData)\n",
    "    fileData.close()\n",
    "        \n",
    "# Append the df created in the cells above to either\n",
    "# (1) the empty projectData dataframe, or (2) the current state of the data as it has been loaded.\n",
    "projectData = projectData.append(df)\n",
    "with open(\"ProjectData.pkl\", \"wb\") as fileData:\n",
    "    pkl.dump(projectData, fileData)\n",
    "fileData.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Curent State of the Dataset, and Create a Backup Copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 4)\n",
      "               School                                               Text  \\\n",
      "0  Indiana University  RT @Prospects_IN: Indiana Prospects Scout Day ...   \n",
      "1  Indiana University  RT @loganmichael99: Excited to announce that I...   \n",
      "2  Indiana University  RT @Hgrooo8: Very blessed and humbled to annou...   \n",
      "3  Indiana University  Thoughts and prayers to our sisters at Indiana...   \n",
      "4  Indiana University  RT @MPA_Vikings: @hordefb with the win over In...   \n",
      "\n",
      "  Location                Time  \n",
      "0     None 2017-09-26 02:30:08  \n",
      "1     None 2017-09-26 02:27:22  \n",
      "2     None 2017-09-26 02:26:24  \n",
      "3     None 2017-09-26 02:17:38  \n",
      "4     None 2017-09-26 02:11:10  \n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset is intact.\n",
    "with open(\"ProjectData.pkl\", \"rb\") as fileData:\n",
    "    accumulated_dataset = pkl.load(fileData)\n",
    "fileData.close()\n",
    "    \n",
    "print(accumulated_dataset.shape)\n",
    "print(accumulated_dataset.head(5))\n",
    "\n",
    "# Overwrite the backup dataset with the full current dataset.\n",
    "with open(\"BackupDataset.pkl\", \"wb\") as backup:\n",
    "    pkl.dump(accumulated_dataset, backup)\n",
    "backup.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And... obsessively check the backup dataset to make sure it is alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 4)\n",
      "               School                                               Text  \\\n",
      "0  Indiana University  RT @Prospects_IN: Indiana Prospects Scout Day ...   \n",
      "1  Indiana University  RT @loganmichael99: Excited to announce that I...   \n",
      "2  Indiana University  RT @Hgrooo8: Very blessed and humbled to annou...   \n",
      "3  Indiana University  Thoughts and prayers to our sisters at Indiana...   \n",
      "4  Indiana University  RT @MPA_Vikings: @hordefb with the win over In...   \n",
      "\n",
      "  Location                Time  \n",
      "0     None 2017-09-26 02:30:08  \n",
      "1     None 2017-09-26 02:27:22  \n",
      "2     None 2017-09-26 02:26:24  \n",
      "3     None 2017-09-26 02:17:38  \n",
      "4     None 2017-09-26 02:11:10  \n"
     ]
    }
   ],
   "source": [
    "with open(\"BackupDataset.pkl\", \"rb\") as checkBackup:\n",
    "    bup = pkl.load(checkBackup)\n",
    "checkBackup.close()\n",
    "print(bup.shape)\n",
    "print(bup.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
